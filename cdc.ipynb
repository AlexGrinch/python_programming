{
 "metadata": {
  "name": "",
  "signature": "sha256:0979279ce502e165e17a2e41bd849c69a8672ea04ec256fe45dcdc6b53fb7240"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Context document clustering"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Imports"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import log\n",
      "import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading and preprocessing data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('docword.nips.txt', 'r')\n",
      "col_size = int(f.readline())\n",
      "voc_size = int(f.readline())\n",
      "f.readline()\n",
      "\n",
      "word_doc = [[] for i in range(voc_size+1)]\n",
      "word_freq = [[] for i in range(voc_size+1)]\n",
      "doc_freq = {}\n",
      "word_doc_size = {}\n",
      "for i in range(col_size+1):\n",
      "    doc_freq[i] = 0\n",
      "    \n",
      "for line in f:\n",
      "    t = [int(s) for s in line.split()]      \n",
      "    word_doc[t[1]].append(t[0])\n",
      "    word_freq[t[1]].append(t[2])\n",
      "    doc_freq[t[0]] += t[2]\n",
      "    \n",
      "for w in range(voc_size+1):\n",
      "    S = 0\n",
      "    for d in word_doc[w]:\n",
      "        S += doc_freq[d]\n",
      "    word_doc_size[w] = S\n",
      "\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Probability distribution and entropy calculation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which calculates probability distribution $p(u|w)$ for two words $w \\in W,~u \\in W$ from collection vocabulary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def p_u_w(u, w):\n",
      "    \n",
      "    idx_u = 0\n",
      "    idx_w = 0\n",
      "    nom = 0\n",
      "    \n",
      "    while(idx_u < len(word_doc[u]) and idx_w < len(word_doc[w])):\n",
      "        if (word_doc[u][idx_u] < word_doc[w][idx_w]):\n",
      "            idx_u += 1\n",
      "        elif (word_doc[u][idx_u] > word_doc[w][idx_w]):\n",
      "            idx_w += 1\n",
      "        else:\n",
      "            nom += word_freq[u][idx_u]\n",
      "            idx_u += 1\n",
      "            idx_w += 1\n",
      "    denom = word_doc_size[w]\n",
      "    if (denom == 0):\n",
      "        return 0\n",
      "    return nom/float(denom)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which calculates entropy $H(w)$ of word $w \\in W$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def entropy(w):\n",
      "    H_w = 0\n",
      "    for u in range(1,voc_size+1):\n",
      "        x = p_u_w(u, w)\n",
      "        if (x > 0):\n",
      "            H_w += - log(x,2) * x\n",
      "    return H_w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which calculates document frequencies $df(w)$ entropies $H(w)$ for the whole collection."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calculate_df_entropy():\n",
      "    print datetime.datetime.now().time()\n",
      "    f = open('df_entropy.nips.txt', 'w')\n",
      "    for w in range(1, voc_size+1):\n",
      "        strr = str(len(word_freq[w])) + ',' + str(entropy(w))\n",
      "        f.write(strr)\n",
      "        f.write('\\n')\n",
      "    f.close()\n",
      "    print datetime.datetime.now().time()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "calculate_df_entropy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "17:23:48.999000\n",
        "20:53:39.758000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading vocabulary and entropy data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which load vocabulary from file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('entropy.nips.txt', 'r')\n",
      "entropy = {}\n",
      "for i in range(voc_size+1):\n",
      "    entropy[i] = 0\n",
      "for (i, line) in enumerate(f, 1):\n",
      "    H_w = float(line) \n",
      "    if (H_w == 0):\n",
      "        entropy[i] = 100\n",
      "    else:\n",
      "        entropy[i] = H_w\n",
      "entropy[0] = 100\n",
      "f.close() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which load entropy from file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('vocab.nips.txt', 'r')\n",
      "vocab = []\n",
      "for line in f: \n",
      "    vocab.append(line.split()[0])\n",
      "f.close() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Vocabulary partition and narrow contexts selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which divide vocabulary into parts $W_i$ based on document frequency $N_w$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def df_partition(df_0, alpha):\n",
      "    Y = [{} for i in range(100)]\n",
      "    size_Y = 0\n",
      "    for w in range(1, voc_size+1):\n",
      "        df = df_0\n",
      "        idx = 1\n",
      "        while (len(word_freq[w]) > df):\n",
      "            df = int(df*alpha)\n",
      "            idx += 1\n",
      "        size_Y = max(size_Y, idx)    \n",
      "        Y[idx][w] = entropy[w]\n",
      "    size_Y += 1\n",
      "    return Y, size_Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which finds topX words within each $W_i$ with the smallest entropy. For standard cdc method of narrow contexts selection $\\textbf{mode}$ should be set to \"cdc\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def topX(num, mode = \"top\", df0 = 6, alpha = 2):\n",
      "    Y, size_Y = df_partition(df0, alpha)\n",
      "    tops = [[] for i in range(size_Y)]\n",
      "    for y in range(1, size_Y):\n",
      "        top_x = sorted(Y[y].items(), key=lambda x: x[1])       \n",
      "        if (mode == \"cdc\"):\n",
      "            n = int((num*len(Y[y]))/float(voc_size))\n",
      "        else:\n",
      "            n = num\n",
      "        top = top_x[0:n]\n",
      "        tops[y] = top\n",
      "    return tops"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which writes all topX words into the file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_topX(num, mode = \"top\", df0 = 6, alpha = 2):\n",
      "    f = open('topX.nips.txt', 'w')\n",
      "    X = topX(num, mode, df0, alpha)\n",
      "    left = 0\n",
      "    right = df0\n",
      "    for y in range(1, len(X)):\n",
      "        f.write(\"Document frequency from \" + str(int(left)) + \" to \" + str(int(right)))\n",
      "        f.write('\\n')\n",
      "        left = right\n",
      "        right *= alpha\n",
      "        idx = 0\n",
      "        for u in X[y]:\n",
      "            w = u[0]\n",
      "            #strr = str(w) + ' ' + str(vocab[w-1])\n",
      "            if (idx == 5):\n",
      "                idx = 0\n",
      "                f.write('\\n')\n",
      "            idx += 1\n",
      "            f.write(vocab[w-1] + '  ')\n",
      "            #f.write('\\n')\n",
      "        f.write('\\n' + '\\n')\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_topX(200, \"cdc\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which writes words, selected as narrow contexts to display them on $H(N_w)$ plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_contexts(num, mode = \"top\", df0 = 6, alpha = 2):\n",
      "    f = open('narcon.nips.csv', 'w')\n",
      "    X = topX(num, mode, df0, alpha)\n",
      "    f.write(\"df,entropy\")\n",
      "    f.write('\\n')\n",
      "    for y in range(1, len(X)):\n",
      "        for u in X[y]:\n",
      "            strr = str(len(word_doc[u[0]])) + ',' + str(u[1])\n",
      "            f.write(strr)\n",
      "            f.write('\\n')\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_contexts(200, \"cdc\", df0 = 6, alpha = 1.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which compares two narrow contexts sets, given from two different document freqency partitions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare(num, mode = \"top\", df1 = 6, alpha1 = 2, df2 = 6, alpha2 = 1.5):\n",
      "    X1 = topX(num, mode, df1, alpha1)\n",
      "    X2 = topX(num, mode, df2, alpha2)\n",
      "    Y1 = []\n",
      "    Y2 = []\n",
      "    first_second = []\n",
      "    second_first = []\n",
      "    for y in range(1, len(X1)):\n",
      "        for u in X1[y]:\n",
      "            Y1.append(u[0])\n",
      "    for y in range(1, len(X2)):\n",
      "        for u in X2[y]:\n",
      "            Y2.append(u[0])\n",
      "    for w in Y1:\n",
      "        flag = 0\n",
      "        for u in Y2:\n",
      "            if (w == u):\n",
      "                flag = 1\n",
      "                break\n",
      "        if (flag == 0):\n",
      "            first_second.append(w)\n",
      "    for w in Y2:\n",
      "        flag = 0\n",
      "        for u in Y1:\n",
      "            if (w == u):\n",
      "                flag = 1\n",
      "                break\n",
      "        if (flag == 0):\n",
      "            second_first.append(w)\n",
      "    return first_second, second_first"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which writes differences between two different document frequency partitions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_diff(num, mode = \"top\", df1 = 6, alpha1 = 2, df2 = 6, alpha2 = 1.5):\n",
      "    f = open('diff.nips.txt', 'w')\n",
      "    Y1, Y2 = compare(num, mode, df1, alpha1, df2, alpha2)\n",
      "    f.write(\"Narrow contexts for alpha = \"+str(alpha1)+\" which are not for alpha = \"+str(alpha2) + '\\n')\n",
      "    f.write('\\n')\n",
      "    idx = 0\n",
      "    for w in Y1:\n",
      "        f.write(vocab[w-1] + '  ')\n",
      "        idx += 1\n",
      "        if (idx == 5):\n",
      "            idx = 0\n",
      "            f.write('\\n')\n",
      "    f.write('\\n' + '\\n' + '\\n')\n",
      "    f.write(\"Narrow contexts for alpha = \"+str(alpha2)+\" which are not for alpha = \"+str(alpha1) + '\\n')\n",
      "    f.write('\\n')\n",
      "    idx = 0\n",
      "    for w in Y2:\n",
      "        f.write(vocab[w-1] + '  ')\n",
      "        idx += 1\n",
      "        if (idx == 5): \n",
      "            idx = 0\n",
      "            f.write('\\n')\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_diff(200, \"cdc\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function, which display top10 words from context of word $w \\in W$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def top10context(w):\n",
      "    context = {}\n",
      "    for u in range(1,voc_size+1):\n",
      "        context[u] = p_u_w(u, w)\n",
      "    context = sorted(context.items(), key=lambda x: x[1], reverse = True)\n",
      "    top10 = context[0:10]\n",
      "    for i in range(10):\n",
      "        print (vocab[top10[i][0]]), top10[i][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top10context(1651)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "functional 0.0171167410077\n",
        "netxvork 0.016215859902\n",
        "learnt 0.011425809145\n",
        "boundaries 0.011228054756\n",
        "setpoint 0.0104150644899\n",
        "numbered 0.00922853815561\n",
        "dimensional 0.00828371163016\n",
        "neurally 0.00806398453121\n",
        "trajec 0.00718507613544\n",
        "inquiry 0.00714113071565\n"
       ]
      }
     ],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}